{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File reduction\n",
    "Since the original file is over 5 GB, pandas struggles to open it all at once on a typical laptop with 8 GB RAM. \n",
    "As a solution, this program cuts it into chunks, groups events by the second (we don't care about millisecond precision of the event) and saves a new file.\n",
    "\n",
    "If you set the chunk size to your liking, make sure that we get the same amount of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HWIImv-32rlP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoupling_time(r):\n",
    "    '''\n",
    "    This will output in this format: 1970-08-22 19:25:00 \n",
    "    '''\n",
    "    s = str(r)\n",
    "    year, month, day, hour, minute, second = s[:4], s[4:6], s[6:8], s[8:10], s[10:12], s[12:14]\n",
    "    \n",
    "    return '-'.join([year, month, day]) + ' ' + ':'.join([hour, minute, second])\n",
    "#decoupling_time(20201117235750093)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-gBp1VMc1s7f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [02:44,  8.03s/it]/tmp/ipykernel_37307/703943581.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  shortened_times = pd.to_datetime(chunk['timestamp'].apply(\n",
      "44it [05:59,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were originally about 132,000,000 rows. Now there are exactly 19,859,174\n",
      "File reduction is completed.\n",
      "The file was saved onto the current directory.\n"
     ]
    }
   ],
   "source": [
    "filename = \"Raw Data/touchevent.csv\"\n",
    "final_data = pd.DataFrame()\n",
    "#different chunk sizes, one_hundred is for tests. \n",
    "five_million = 5*10**6\n",
    "three_million = 3*10**6\n",
    "one_hundred = 10**2\n",
    "c = 0\n",
    "for chunk in tqdm(pd.read_csv(filename, chunksize=three_million)): \n",
    "    #deep copy \n",
    "    temp = chunk[['userid','day']]\n",
    "    \n",
    "    #turning now time from raw to machine readable\n",
    "    shortened_times = pd.to_datetime(chunk['timestamp'].apply(\n",
    "        decoupling_time), yearfirst=True).dt.floor('s')\n",
    "    temp.insert(2, 'timestamp', shortened_times)\n",
    "    \n",
    "    #grouping \n",
    "    df_grouped = temp.groupby(['userid', 'day','timestamp' ]).size()                                         \n",
    "    df_grouped = df_grouped.reset_index(name='touches')\n",
    "    \n",
    "    #making the day machine readable\n",
    "    df_grouped.day = df_grouped.day.apply(lambda x: str(x)[:4]+'/'+str(x)[4:6]+'/'+str(x)[6:])\n",
    "    df_grouped.day = pd.to_datetime(df_grouped.day, yearfirst=True)\n",
    "    \n",
    "    # ONLY NOVEMBER \n",
    "    '''\n",
    "    df_grouped.day = df_grouped.day.apply(lambda x: True if str(x)[4:6] == \"11\" else False) #only november \n",
    "    df_grouped.rename(columns={\"day\": \"first_two_weeks\"}, inplace=True)\n",
    "    df_grouped = df_grouped[df_grouped.first_two_weeks]   #this does the selection\n",
    "    '''\n",
    "    #df_grouped = df_grouped.drop(['first_two_weeks'], axis='columns')\n",
    "    \n",
    "    #appending to a global df\n",
    "    final_data = pd.concat([final_data, df_grouped], ignore_index=True) \n",
    "    c += 1  #break during tests\n",
    "    if c == 100:\n",
    "        break\n",
    "print(f\"There were originally about {c*three_million:,} rows. Now there are exactly {final_data.shape[0]:,}\")\n",
    "print(\"File reduction is completed.\")\n",
    "final_data.to_csv('Processed Data/touch_sensor_unclean.csv')\n",
    "print('The file was saved onto the current directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, most of this cell has to be unblocked\n",
    "# and needs to be put hashtags all over the \"ONLY FIRST TWO WEEKS\" block in the previous for loop.\n",
    "\n",
    "days_in_total = final_data.day.value_counts().sort_index()\n",
    "raw_data_days_frequency.to_csv('Processed Data/total_days_and_frequency.csv')\n",
    "table=days_in_total\n",
    "#table = pd.read_csv(\"Processed Data/total_days_and_frequency.csv\").sort_index()\n",
    "g = sns.barplot(data=table, x=table['day'], y=table['count'], color='#977dc7')\n",
    "g.set_xticklabels(labels=table.day.apply(lambda x: str(x)[-2:] + '/' + str(x)[-5:-3]),rotation=90)\n",
    "# It seems like from 12th of November 'till 11th of December there was most activity. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure there are no mistakes.\n",
    "ATTENTION: ONLY DO THIS IF YOU SET chunk size SMALLER THAN THREE MILLION ROWS\n",
    "\n",
    "When grouping by second, because of the massive size of the original dataset, there can be mistakes when passing from one iteration to the other. For example, two observations with the same second but parsed in two different chunks for chance; this means that they don't get grouped in the same chunk and it will result in two different rows. \n",
    "The fastest solution is to check if the count of rows is right, then re-run the code on the parsed, processed new file as this time no _for_ loop is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "filename = 'Processed Data/touch_sensor_unclean.csv'\n",
    "by_second_unclean = pd.read_csv(filename)\n",
    "by_second_unclean.drop('Unnamed: 0', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1970-08-22 22:19:25'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoupling_time1(r):\n",
    "    '''\n",
    "    This will output in this format: 1970-08-22 19:25:00 \n",
    "    '''\n",
    "    s = str(r)\n",
    "    year, month, day, hour, minute, second = s[:4], s[5:7], s[8:10], s[8:10], s[11:13], s[14:16]\n",
    "    \n",
    "    return '-'.join([year, month, day]) + ' ' + ':'.join([hour, minute, second])\n",
    "decoupling_time1(\"1970-08-22 19:25:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if by_second_unclean.shape[0] <= 19859174: #it should be this long, exactly every row; \n",
    "    #I have the solution in the case it's longer. \n",
    "    None #the count is right. \n",
    "elif by_second_unclean.shape[0] > 19859174:   \n",
    "    temp = by_second_unclean[['userid','day']]\n",
    "    shortened_times = pd.to_datetime(chunk['timestamp'].apply(decoupling_time1), \n",
    "                                     yearfirst=True).dt.floor('s')\n",
    "    \n",
    "    temp.insert(2, 'timestamp', shortened_times)\n",
    "    \n",
    "    df_grouped = temp.groupby(['userid', 'day', 'timestamp']).size()                                         \n",
    "    df_grouped = df_grouped.reset_index(name='touches')\n",
    "# I have no solution if it's shorter. Play with the chunk size until you get the correct result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome:\n",
      "19,859,174\n"
     ]
    }
   ],
   "source": [
    "print('outcome:')\n",
    "try:\n",
    "    print(f\"{df_grouped.shape[0]:,}\")\n",
    "    df_grouped.to_csv(filename)\n",
    "except NameError: \n",
    "    print(f\"{by_second_unclean.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
